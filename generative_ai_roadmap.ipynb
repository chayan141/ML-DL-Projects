{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSugQag6Lnj46HedLDn/S4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Transformers, GPT Models, GEN AI Apps, Gemini APPs\n","Open Source LLMs with Hugging Face, Gradio Deployment, RAG, Ollama, Local RAG Application with streamlit."],"metadata":{"id":"eSHacCBPgebu"}},{"cell_type":"markdown","source":["Transformers : A transformer is a neural network that learns the context of sequential data and generates new data out of it.\n","\n","Transforms input sequence to the output sequence.\n","\n","1. Self Attention\n","2. Positional Encoding\n","3. Parallel Processing\n","4. Encoder-Decoder Architecture"],"metadata":{"id":"1uV9qAqEhJdC"}},{"cell_type":"markdown","source":["# Word Embeddings"],"metadata":{"id":"Csd7bz2WpJDL"}},{"cell_type":"markdown","source":["Word embedding is a technique in natural language processing (NLP) where words are represented as dense vectors of real numbers. These vectors capture semantic relationships between words, meaning that words with similar meanings have similar vector representations. Word embeddings are essential for many NLP tasks, such as text classification, machine translation, and sentiment analysis."],"metadata":{"id":"qLLliI3dpRce"}},{"cell_type":"markdown","source":["***Training Methods:***\n","\n","-Word2Vec: Predicts context words given a target word (Skip-gram) or predicts a target word given context words (CBOW).\n","\n","GloVe (Global Vectors for Word Representation): Uses word co-occurrence statistics from a corpus to create embeddings."],"metadata":{"id":"iSimP5zmpfhr"}},{"cell_type":"markdown","source":["***Positional Encoder :***\n","\n","The vector that gives context based on position of word in sentence.\n","\n","It helps the model understand the order or position of words within a sequence. Unlike traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), which inherently capture sequential information through their architectures, Transformer models process input sequences in parallel. This parallel processing means that transformers do not have a built in mechenism to recognise the order of tokens in the input sequence. The positional encoder addresses this issue by encoding the positional information.\n","\n","**Introduce Positional Information:** The positional encoder adds information about the position of each token in the sequence. This helps the model differentiate between sequences with the same tokens but in different orders.\n","\n","**Enable Sequence Order Understanding:** By adding positional encodings to the token embeddings, the model can leverage both the token's identity and its position within the sequence.\n","\n","**Support Parallel Processing:** Since Transformer models process all tokens in parallel, the positional encoding ensures that the model can still consider the order of tokens without relying on sequential operations."],"metadata":{"id":"k3V1-ouGqklH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xmd8Nq2_gaHW"},"outputs":[],"source":[]}]}